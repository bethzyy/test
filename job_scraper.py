import requests
from bs4 import BeautifulSoup
import re

# å®šä¹‰è½¯ä»¶å¼€å‘ç›¸å…³å…³é”®è¯ - æ‰©å±•æ›´å¤šç›¸å…³å…³é”®è¯
KEYWORDS = [
    # åŸºç¡€å¼€å‘è¯æ±‡
    'è½¯ä»¶å¼€å‘', 'è½¯ä»¶å·¥ç¨‹å¸ˆ', 'ç¨‹åºå‘˜', 'å¼€å‘è€…', 'å¼€å‘å·¥ç¨‹å¸ˆ',
    
    # å‰åç«¯å¼€å‘
    'å‰ç«¯å¼€å‘', 'åç«¯å¼€å‘', 'å…¨æ ˆå¼€å‘', 'Webå¼€å‘', 'ç½‘é¡µå¼€å‘',
    'Javaå¼€å‘', 'Pythonå¼€å‘', 'C++å¼€å‘', 'C#å¼€å‘', 'PHPå¼€å‘',
    'JavaScriptå¼€å‘', 'Reactå¼€å‘', 'Vueå¼€å‘', 'Node.jså¼€å‘',
    
    # ç§»åŠ¨å¼€å‘
    'Androidå¼€å‘', 'iOSå¼€å‘', 'ç§»åŠ¨å¼€å‘', 'APPå¼€å‘', 'å°ç¨‹åºå¼€å‘',
    
    # æ•°æ®åº“å’Œè¿ç»´
    'æ•°æ®åº“å¼€å‘', 'è¿ç»´å¼€å‘', 'DevOps', 'ç³»ç»Ÿè¿ç»´', 'æ•°æ®åº“ç®¡ç†å‘˜',
    
    # äººå·¥æ™ºèƒ½å’Œå¤§æ•°æ®
    'AIå¼€å‘', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'æ•°æ®ç§‘å­¦',
    'å¤§æ•°æ®å¼€å‘', 'æ•°æ®åˆ†æ', 'ç®—æ³•å·¥ç¨‹å¸ˆ', 'æ™ºèƒ½ä½“å¼€å‘',
    
    # äº‘è®¡ç®—å’Œå¾®æœåŠ¡
    'äº‘è®¡ç®—', 'å¾®æœåŠ¡', 'å®¹å™¨å¼€å‘', 'Kubernetes', 'Dockerå¼€å‘',
    
    # å…·ä½“æŠ€æœ¯æ ˆ
    'Springå¼€å‘', 'Djangoå¼€å‘', 'Flaskå¼€å‘', 'Laravelå¼€å‘',
    'ç¼–ç¨‹', 'ä»£ç å¼€å‘', 'ç¨‹åºå¼€å‘', 'åº”ç”¨å¼€å‘', 'ç³»ç»Ÿå¼€å‘',
    
    # æ¶æ„å’Œé«˜çº§èŒä½
    'æ¶æ„å¸ˆ', 'æŠ€æœ¯æ€»ç›‘', 'CTO', 'æŠ€æœ¯è´Ÿè´£äºº', 'é¦–å¸­æŠ€æœ¯å®˜',
    
    # æµ‹è¯•å’Œè´¨é‡ä¿è¯
    'æµ‹è¯•å¼€å‘', 'è‡ªåŠ¨åŒ–æµ‹è¯•', 'QAå·¥ç¨‹å¸ˆ', 'è´¨é‡ä¿éšœ'
]

def read_job_urls(file_path):
    """è¯»å–èŒä½URLæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return [url.strip() for url in f if url.strip()]
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return []

def fetch_job_page(url):
    """è·å–èŒä½é¡µé¢å†…å®¹ï¼Œå¤„ç†å¼‚å¸¸"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        return response.text
    except requests.exceptions.Timeout:
        print(f"â±ï¸  è®¿é—®è¶…æ—¶: {url}")
        return None
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTPé”™è¯¯ {e.response.status_code}: {url}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"âŒ è®¿é—®å¤±è´¥: {url} - {e}")
        return None

def parse_job_info(html, url):
    """è§£ææ™ºè”æ‹›è˜é¡µé¢çš„èŒä½ä¿¡æ¯"""
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ“ æ­£åœ¨è§£æé¡µé¢: {url}")
        
        # æå–èŒä½åç§°ï¼ˆæ™ºè”æ‹›è˜çš„èŒä½åç§°åœ¨ä¸åŒé¡µé¢ç»“æ„å¯èƒ½ä¸åŒï¼‰
        job_title = "æœªçŸ¥èŒä½"
        
        # å°è¯•ä¸åŒçš„èŒä½åç§°é€‰æ‹©å™¨ - æ‰©å±•æ›´å¤šé€‰æ‹©å™¨
        title_selectors = [
            'h1[class*="name"]',  # å¸¸è§çš„èŒä½åç§°é€‰æ‹©å™¨
            'h1[class*="title"]',
            'div[class*="job-name"] h1',
            'div[class*="job-title"] h1',
            'h1[class="zp-job-name__company"]',  # ç‰¹å®šé¡µé¢çš„é€‰æ‹©å™¨
            'div[class*="job-position"] h1',
            'h1[class*="position"]',
            'div[class*="position-name"]',
            'div[class*="jobName"]',
            'h1[class*="job"]',
            'title'  # æœ€åå°è¯•ä»é¡µé¢æ ‡é¢˜æå–
        ]
        
        for selector in title_selectors:
            title_element = soup.select_one(selector)
            if title_element:
                if selector == 'title':
                    # ä»é¡µé¢æ ‡é¢˜æå–ï¼Œé€šå¸¸æ ¼å¼ä¸º "èŒä½åç§°_å…¬å¸åç§°-æ™ºè”æ‹›è˜"
                    full_title = title_element.get_text(strip=True)
                    # ç§»é™¤åç¼€
                    job_title = re.split(r'[_-]', full_title)[0].strip()
                else:
                    job_title = title_element.get_text(strip=True)
                break
        
        # å¦‚æœè¿˜æ˜¯æœªçŸ¥èŒä½ï¼Œå°è¯•ä»URLæˆ–é¡µé¢å†…å®¹ä¸­æå–æ›´å¤šä¿¡æ¯
        if job_title == "æœªçŸ¥èŒä½":
            print(f"âš ï¸  æœªæ‰¾åˆ°æ ‡å‡†èŒä½åç§°ï¼Œå°è¯•æ™ºèƒ½æå–...")
            
            # å°è¯•ä»URLä¸­æå–èŒä½ä¿¡æ¯
            url_parts = url.split('/')
            for part in reversed(url_parts):
                # æŸ¥æ‰¾åŒ…å«èŒä½IDçš„éƒ¨åˆ†ï¼Œé€šå¸¸æ ¼å¼ä¸º CCL... æˆ– CC...
                if re.match(r'^[A-Z]{2,3}\d+J\d+', part):
                    # è¿™éƒ¨åˆ†é€šå¸¸æ˜¯èŒä½IDï¼Œä¸åŒ…å«èŒä½åç§°ï¼Œè·³è¿‡
                    continue
                elif len(part) > 5 and not part.startswith('http') and '.' not in part:
                    # å°è¯•ä»URLè·¯å¾„ä¸­æå–èŒä½ä¿¡æ¯ï¼Œæ¸…ç†ç‰¹æ®Šå­—ç¬¦
                    job_info = re.sub(r'[^\u4e00-\u9fa5a-zA-Z\s]', ' ', part)
                    job_info = re.sub(r'\s+', ' ', job_info).strip()
                    if len(job_info) > 5 and 'htm' not in job_info.lower():
                        job_title = job_info
                        print(f"ğŸ“‹ ä»URLè·¯å¾„æå–èŒä½: {job_title}")
                        break
            
            # å°è¯•ä»metaæ ‡ç­¾ä¸­è·å–
            if job_title == "æœªçŸ¥èŒä½":
                meta_title = soup.find('meta', {'property': 'og:title'}) or soup.find('meta', {'name': 'title'})
                if meta_title and meta_title.get('content'):
                    content = meta_title['content']
                    # å°è¯•ä»metaå†…å®¹ä¸­æå–èŒä½åç§°
                    if 'æ‹›è˜' in content:
                        job_title = content.split('æ‹›è˜')[0].strip()
                    elif '_' in content:
                        job_title = content.split('_')[0].strip()
                    elif '-' in content:
                        job_title = content.split('-')[0].strip()
                    else:
                        job_title = content.strip()
                    print(f"ğŸ“‹ ä»metaæ ‡ç­¾æå–èŒä½: {job_title}")
            
            # å¦‚æœè¿˜æ˜¯æœªçŸ¥ï¼Œå°è¯•ä»æ•´ä¸ªé¡µé¢æ–‡æœ¬ä¸­æ™ºèƒ½æå–
            if job_title == "æœªçŸ¥èŒä½":
                # æŸ¥æ‰¾åŒ…å«"æ‹›è˜"æˆ–"èŒä½"çš„æ–‡æœ¬
                text_content = soup.get_text()
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾å¯èƒ½çš„èŒä½åç§°
                job_patterns = [
                    r'ã€Œ([^ã€]*?)æ‹›è˜ã€',
                    r'ã€Œ([^ã€]*?)ã€',
                    r'([^ï¼Œã€‚ï¼ï¼Ÿ\n]*?å·¥ç¨‹å¸ˆ[^ï¼Œã€‚ï¼ï¼Ÿ\n]*)',
                    r'([^ï¼Œã€‚ï¼ï¼Ÿ\n]*?å¼€å‘[^ï¼Œã€‚ï¼ï¼Ÿ\n]*)',
                    r'([^ï¼Œã€‚ï¼ï¼Ÿ\n]*?ç¨‹åºå‘˜[^ï¼Œã€‚ï¼ï¼Ÿ\n]*)',
                    r'([^ï¼Œã€‚ï¼ï¼Ÿ\n]*?æ¶æ„å¸ˆ[^ï¼Œã€‚ï¼ï¼Ÿ\n]*)',
                    r'([^ï¼Œã€‚ï¼ï¼Ÿ\n]*?æŠ€æœ¯[^ï¼Œã€‚ï¼ï¼Ÿ\n]*)',
                    r'([^ï¼Œã€‚ï¼ï¼Ÿ\n]*?è½¯ä»¶[^ï¼Œã€‚ï¼ï¼Ÿ\n]*)'
                ]
                
                for pattern in job_patterns:
                    match = re.search(pattern, text_content)
                    if match:
                        job_title = match.group(1) if match.groups() else match.group(0)
                        # æ¸…ç†æå–çš„èŒä½åç§°
                        job_title = re.sub(r'[\s\n\r]+', ' ', job_title).strip()
                        if len(job_title) > 2 and len(job_title) < 50:  # åˆç†çš„èŒä½åç§°é•¿åº¦
                            print(f"ğŸ“‹ ä»é¡µé¢æ–‡æœ¬æ™ºèƒ½æå–èŒä½: {job_title}")
                            break
                        else:
                            job_title = "æœªçŸ¥èŒä½"
        
        print(f"ğŸ“‹ æå–åˆ°çš„èŒä½åç§°: {job_title}")
        
        # æå–èŒä½æè¿°å’Œè¦æ±‚
        job_content = ""
        
        # æ™ºè”æ‹›è˜çš„èŒä½æè¿°é€šå¸¸åœ¨ä»¥ä¸‹ä½ç½® - æ‰©å±•æ›´å¤šé€‰æ‹©å™¨
        description_selectors = [
            'div[class*="describtion__detail-content"]',  # æ–°é¡µé¢ç»“æ„
            'div[class="describtion__detail-content"]',
            'div[class*="job-description"]',
            'div[class*="job-detail-content"]',
            'div[class*="pos-ul"]',
            'section[class*="job-intro"]',
            'div[class="pos-ul"]',
            'div[class="responsibility-req"]',  # æ—§é¡µé¢ç»“æ„
            'div[class*="job-detail"]',
            'div[class*="position-desc"]',
            'div[class*="job-desc"]',
            'div[class*="detail-content"]',
            'div[class*="job-requirement"]',
            'div[class*="position-require"]',
            'div[class*="job-content"]',
            'div[class*="position-detail"]'
        ]
        
        for selector in description_selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    # è·å–æ–‡æœ¬ä½†æ’é™¤è„šæœ¬å’Œæ ·å¼å†…å®¹
                    for script in element(["script", "style"]):
                        script.decompose()
                    job_content += element.get_text(separator='\n', strip=True) + '\n'
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ›´æ™ºèƒ½çš„å†…å®¹æå–
        if not job_content:
            print(f"âš ï¸  æœªæ‰¾åˆ°æ ‡å‡†èŒä½æè¿°ï¼Œå°è¯•æ™ºèƒ½æå–...")
            # å°è¯•æŸ¥æ‰¾åŒ…å«èŒä½æè¿°å…³é”®è¯çš„div
            desc_keywords = ['èŒä½æè¿°', 'å²—ä½èŒè´£', 'ä»»èŒè¦æ±‚', 'å·¥ä½œè¦æ±‚', 'å·¥ä½œå†…å®¹', 'èŒè´£æè¿°']
            for keyword in desc_keywords:
                # æŸ¥æ‰¾åŒ…å«è¿™äº›å…³é”®è¯çš„å…ƒç´  - ä½¿ç”¨stringä»£æ›¿å·²å¼ƒç”¨çš„textå‚æ•°
                elements = soup.find_all(string=re.compile(keyword))
                for element in elements:
                    parent = element.find_parent(['div', 'section', 'p'])
                    if parent:
                        job_content += parent.get_text(separator='\n', strip=True) + '\n'
                        break
                if job_content:
                    print(f"ğŸ“„ ä»å…³é”®è¯'{keyword}'æå–åˆ°èŒä½æè¿°")
                    break
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè·å–é¡µé¢çš„ä¸»è¦å†…å®¹åŒºåŸŸ
        if not job_content:
            print(f"âš ï¸  ä»æœªæ‰¾åˆ°èŒä½æè¿°ï¼Œå°è¯•æå–ä¸»è¦å†…å®¹åŒºåŸŸ...")
            # å°è¯•å¤šç§ä¸»è¦å†…å®¹åŒºåŸŸé€‰æ‹©å™¨
            main_selectors = [
                'main',
                'div[class*="main"]',
                'div[class*="content"]',
                'div[class*="body"]',
                'div[class*="wrapper"]',
                'div[class*="container"]',
                'div[id*="main"]',
                'div[id*="content"]',
                'section[class*="content"]',
                'article[class*="content"]'
            ]
            
            main_content = None
            for selector in main_selectors:
                if selector == 'main':
                    main_content = soup.find('main')
                else:
                    main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if main_content:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                for script in main_content(["script", "style"]):
                    script.decompose()
                job_content = main_content.get_text(separator='\n', strip=True)
                print(f"ğŸ“„ ä»ä¸»è¦å†…å®¹åŒºåŸŸæå–åˆ°{len(job_content)}å­—ç¬¦çš„æ–‡æœ¬")
            else:
                # æœ€åæ‰‹æ®µï¼šå°è¯•æ›´æ™ºèƒ½çš„å†…å®¹æå–
                print(f"âš ï¸  å°è¯•æ™ºèƒ½å†…å®¹æå–...")
                
                # å°è¯•å¤šç§æ–¹æ³•æå–å†…å®¹
                job_content = ""
                
                # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰å¯è§æ–‡æœ¬å†…å®¹
                try:
                    # è·å–bodyå†…å®¹æˆ–æ•´ä¸ªæ–‡æ¡£
                    body = soup.find('body') or soup
                    
                    # ç§»é™¤è„šæœ¬ã€æ ·å¼ã€å¯¼èˆªç­‰æ— å…³å†…å®¹
                    for element in body(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                        element.decompose()
                    
                    # æå–æ–‡æœ¬
                    raw_text = body.get_text(separator='\n', strip=True)
                    
                    # æ–¹æ³•2: å¦‚æœå†…å®¹å¤ªå°‘ï¼Œå°è¯•æŸ¥æ‰¾ç‰¹å®šçš„å†…å®¹å®¹å™¨
                    if len(raw_text) < 100:
                        print(f"âš ï¸  å†…å®¹å¤ªå°‘ï¼Œå°è¯•æŸ¥æ‰¾éšè—å†…å®¹...")
                        # æŸ¥æ‰¾å¯èƒ½åŒ…å«å†…å®¹çš„å…ƒç´ ï¼ŒåŒ…æ‹¬éšè—çš„å…ƒç´ 
                        content_elements = soup.find_all(['div', 'section', 'article', 'main'], 
                                                      attrs={'style': re.compile(r'display:\s*block|visibility:\s*visible')})
                        
                        if not content_elements:
                            # å°è¯•æŸ¥æ‰¾æ‰€æœ‰divå…ƒç´ 
                            content_elements = soup.find_all('div', limit=20)
                        
                        extracted_texts = []
                        for elem in content_elements:
                            text = elem.get_text(strip=True)
                            if len(text) > 50:  # åªä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬
                                extracted_texts.append(text)
                        
                        if extracted_texts:
                            raw_text = '\n'.join(extracted_texts[:5])  # é™åˆ¶æ•°é‡
                    
                    # æ–¹æ³•3: æŸ¥æ‰¾JSONæ•°æ®æˆ–è„šæœ¬ä¸­çš„å†…å®¹
                    if len(raw_text) < 50:
                        print(f"âš ï¸  å°è¯•ä»è„šæœ¬ä¸­æå–æ•°æ®...")
                        scripts = soup.find_all('script')
                        for script in scripts:
                            if script.string and len(script.string) > 100:
                                # æŸ¥æ‰¾å¯èƒ½åŒ…å«èŒä½ä¿¡æ¯çš„JSONæ•°æ®
                                if 'job' in script.string.lower() or 'position' in script.string.lower():
                                    # å°è¯•æå–ä¸­æ–‡æ–‡æœ¬
                                    chinese_text = re.findall(r'[\u4e00-\u9fa5]+', script.string)
                                    if chinese_text:
                                        raw_text += ' '.join(chinese_text[:20])  # é™åˆ¶æ•°é‡
                                        break
                    
                    # æ¸…ç†å’Œæ ¼å¼åŒ–æ–‡æœ¬
                    if raw_text:
                        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                        raw_text = re.sub(r'\n{3,}', '\n\n', raw_text)
                        raw_text = re.sub(r'[ \t]+', ' ', raw_text)
                        raw_text = re.sub(r'\s*\n\s*\n\s*', '\n\n', raw_text)
                        
                        # é™åˆ¶é•¿åº¦ä½†ä¿ç•™è¶³å¤Ÿä¿¡æ¯
                        job_content = raw_text[:4000] if len(raw_text) > 4000 else raw_text
                        print(f"ğŸ“„ æ™ºèƒ½æå–æˆåŠŸï¼Œå¾—åˆ°{len(job_content)}å­—ç¬¦çš„æ–‡æœ¬")
                    else:
                        print(f"âš ï¸  æ™ºèƒ½æå–å¤±è´¥ï¼Œå†…å®¹ä¸ºç©º")
                        
                except Exception as e:
                    print(f"âš ï¸  æ™ºèƒ½æå–å‡ºé”™: {e}")
                    # æœ€ç»ˆå¤‡ç”¨æ–¹æ¡ˆ
                    job_content = soup.get_text(separator='\n', strip=True)[:2000]
                    print(f"ğŸ“„ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆæå–åˆ°{len(job_content)}å­—ç¬¦çš„æ–‡æœ¬")
        
        print(f"ğŸ“„ æå–åˆ°çš„èŒä½æè¿°é•¿åº¦: {len(job_content)}å­—ç¬¦")
        
        return {
            'url': url,
            'title': job_title,
            'content': job_content
        }
        
    except Exception as e:
        print(f"ğŸ§© è§£æå¤±è´¥: {url} - {e}")
        return None

def filter_software_jobs(job_info):
    """ç­›é€‰è½¯ä»¶å¼€å‘ç›¸å…³èŒä½"""
    if not job_info:
        return None
    
    print(f"ğŸ” æ­£åœ¨ç­›é€‰èŒä½: {job_info['title']}")
    
    # åˆå¹¶èŒä½åç§°å’Œå†…å®¹è¿›è¡ŒåŒ¹é…
    title_text = job_info['title'].lower()
    content_text = job_info['content'].lower()
    all_text = title_text + ' ' + content_text
    
    # æ£€æŸ¥å…³é”®è¯åŒ¹é…
    matched_keywords = []
    
    # é¦–å…ˆæ£€æŸ¥èŒä½åç§°ä¸­çš„å…³é”®è¯ï¼ˆèŒä½åç§°åŒ¹é…æƒé‡æ›´é«˜ï¼‰
    title_matched = []
    for keyword in KEYWORDS:
        if keyword.lower() in title_text:
            title_matched.append(keyword)
    
    print(f"ğŸ“‹ èŒä½åç§°åŒ¹é…åˆ°çš„å…³é”®è¯: {title_matched}")
    
    # å¦‚æœèŒä½åç§°ä¸­æœ‰æ˜ç¡®çš„æŠ€æœ¯å…³é”®è¯ï¼Œç›´æ¥è®¤ä¸ºåŒ¹é…
    if title_matched:
        print(f"âœ… èŒä½åç§°åŒ¹é…æˆåŠŸï¼")
        return {
            'url': job_info['url'],
            'title': job_info['title'],
            'matched_reason': f"èŒä½åç§°åŒ¹é…å…³é”®è¯: {', '.join(title_matched)}"
        }
    
    # å¦‚æœèŒä½åç§°æ²¡æœ‰æ˜ç¡®åŒ¹é…ï¼Œæ£€æŸ¥å†…å®¹ä¸­çš„å…³é”®è¯
    content_matched = []
    for keyword in KEYWORDS:
        if keyword.lower() in content_text:
            content_matched.append(keyword)
    
    print(f"ğŸ“„ å†…å®¹åŒ¹é…åˆ°çš„å…³é”®è¯: {content_matched}")
    
    # å†…å®¹ä¸­éœ€è¦è‡³å°‘åŒ¹é…1ä¸ªå…³é”®è¯å°±è®¤ä¸ºæ˜¯ç›¸å…³çš„ï¼ˆæ”¾å®½æ¡ä»¶ï¼‰
    if len(content_matched) >= 1:
        print(f"âœ… å†…å®¹åŒ¹é…æˆåŠŸï¼åŒ¹é…åˆ°{len(content_matched)}ä¸ªå…³é”®è¯")
        return {
            'url': job_info['url'],
            'title': job_info['title'],
            'matched_reason': f"å†…å®¹åŒ¹é…å…³é”®è¯: {', '.join(content_matched[:3])}"  # åªæ˜¾ç¤ºå‰3ä¸ªåŒ¹é…çš„å…³é”®è¯
        }
    
    # ç‰¹æ®Šè§„åˆ™ï¼šå¦‚æœå†…å®¹ä¸­åŒ…å«æŸäº›å¼ºç›¸å…³çš„æŠ€æœ¯è¯æ±‡ï¼Œå³ä½¿åªåŒ¹é…ä¸€ä¸ªä¹Ÿè®¤ä¸ºç›¸å…³
    strong_tech_keywords = ['ç¼–ç¨‹', 'ä»£ç ', 'ç®—æ³•', 'æ•°æ®ç»“æ„', 'è½¯ä»¶æ¶æ„', 'ç³»ç»Ÿè®¾è®¡']
    for strong_keyword in strong_tech_keywords:
        if strong_keyword in content_text and content_matched:
            print(f"âœ… å¼ºæŠ€æœ¯è¯æ±‡åŒ¹é…æˆåŠŸï¼")
            return {
                'url': job_info['url'],
                'title': job_info['title'],
                'matched_reason': f"æŠ€æœ¯å†…å®¹åŒ¹é…: {content_matched[0]}"
            }
    
    print(f"â ä¸åŒ¹é…ï¼šèŒä½åç§°æœªåŒ¹é…ï¼Œå†…å®¹ä¸­åŒ¹é…åˆ°{len(content_matched)}ä¸ªå…³é”®è¯ï¼ˆéœ€è¦â‰¥1ä¸ªï¼‰")
    return None

def main():
    """ä¸»å‡½æ•°"""
    file_path = './joburl.txt'
    
    print("ğŸ” å¼€å§‹ç­›é€‰è½¯ä»¶å¼€å‘èŒä½...")
    print("=" * 60)
    
    # è¯»å–URLåˆ—è¡¨
    urls = read_job_urls(file_path)
    if not urls:
        print("ğŸ“ æœªæ‰¾åˆ°èŒä½URL")
        return
    
    print(f"ğŸ“‹ å…±è¯»å–åˆ° {len(urls)} ä¸ªèŒä½URL")
    print("=" * 60)
    
    # å¤„ç†æ¯ä¸ªURL
    matched_jobs = []
    for idx, url in enumerate(urls, 1):
        print(f"ğŸ”— æ­£åœ¨å¤„ç† ({idx}/{len(urls)}): {url}")
        
        html = fetch_job_page(url)
        if html:
            job_info = parse_job_info(html, url)
            if job_info:
                filtered_job = filter_software_jobs(job_info)
                if filtered_job:
                    matched_jobs.append(filtered_job)
                    print(f"âœ… æ‰¾åˆ°åŒ¹é…èŒä½: {job_info['title']}")
                else:
                    print(f"â ä¸åŒ¹é…: {job_info['title']}")
            else:
                print(f"âŒ è§£æå¤±è´¥")
        else:
            print(f"âŒ è®¿é—®å¤±è´¥")
        print("-" * 60)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    if matched_jobs:
        print("ğŸ‰ ç­›é€‰ç»“æœ:")
        print("=" * 60)
        for i, job in enumerate(matched_jobs, 1):
            print(f"èŒä½ {i}:")
            print(f"ğŸ“Œ ç½‘å€: {job['url']}")
            print(f"ğŸ“ åç§°: {job['title']}")
            print(f"ğŸ’¡ åŒ¹é…ç†ç”±: {job['matched_reason']}")
            print("-" * 60)
    else:
        print("âš ï¸  æœªæ‰¾åˆ°æ‹›è˜è½¯ä»¶å¼€å‘äººå‘˜çš„èŒä½")

if __name__ == "__main__":
    main()